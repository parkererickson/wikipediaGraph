{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Classification Using Graph Convolutional Networks and TigerGraph\n",
    "------\n",
    "## Introduction\n",
    "This notebook connects to a TigerGraph database that holds Wikipedia article topics and their links between them, as well as various keywords mentioned in the articles. Check out the README for more information surrounding the graph database end of the project. The purpose of this notebook is to classify quiz-bowl styled questions using a Graph Convolutional Neural Network purely on the links between questions. For a deeper explaination of the GCN, check out [this](https://arxiv.org/abs/1609.02907) link."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Packages\n",
    "The core packages that need to be installed are PyTorch, dgl, and pyTigerGraph. PyTorch and dgl are used for creating and training the GCN, while pyTigerGraph is used for connecting to the TigerGraph database. We also import networkx for converting the list of edges from TigerGraph into a graph dgl can work with. gcn and cfg are other files in the code directory. gcn is already created, but you must create cfg with your TigerGraph API token. The file should look like:\n",
    "```py\n",
    "token = \"YOUR_API_TOKEN_HERE\"\n",
    "```\n",
    "To install the other packages, simply use: ```pip3 install torch dgl networkx pyTigerGraph matplotlib```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyTigerGraph as tg\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "from py_scripts.gcn import GCN\n",
    "from py_scripts.gcn import threeLayerGCN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import py_scripts.cfg as cfg\n",
    "import gensim\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "Here we define some variables, such as the number of epochs of training (usually only need 30 or less for a 2-layer GCN, 100+ for a 3+ layer GCN) and learning rate (0.01 seems to work well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 100\n",
    "learningRate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Database Connection and Creating Edge List\n",
    "This section instantiates a connection to the TigerGraph database and creates a list of tuples with consisting of directed edges in the form of (from, to). This is done through two dictionaries that corresponds an article name to a unique numerical id that is needed to process the graph in the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "219699\n"
    }
   ],
   "source": [
    "conn = tg.TigerGraphConnection(ipAddress=\"https://wikipediagraph.i.tgcloud.us\", apiToken=cfg.token, graphname=\"MyGraph\") # connection to TigerGraph database\n",
    "\n",
    "\n",
    "\n",
    "edges = [createEdge(thing) for thing in conn.runInstalledQuery(\"trainQuestionTuples\", {})[\"results\"][0][\"list_of_question_tuples\"]] # creates list of edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing Graph\n",
    "This section converts the list of edges into a graph that DGL can process in the GCN. It also converts our wanted and unwanted topics to their corresponding numerical ids that we will use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "We have 4588 nodes.\nWe have 1198 edges.\n"
    }
   ],
   "source": [
    "g = nx.DiGraph()\n",
    "\n",
    "g.add_edges_from(edges)\n",
    "\n",
    "G = dgl.DGLGraph(g)\n",
    "\n",
    "print('We have %d nodes.' % G.number_of_nodes())\n",
    "print('We have %d edges.' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Features to Verticies in Graph\n",
    "We will use the doc2vec model to encode each of the article's into a numerical vector that can be used for the features of the vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading pickled features\ntensor([[-15.7746,  -7.8044,  12.4863,  22.5554,   1.3301,  -1.9337,  19.6358,\n          -2.2648, -15.7526, -19.0251, -21.6771, -31.0005,  -6.7516,  23.1438,\n         -26.6049,   3.5942,   8.5233,  50.1070, -11.7487,   4.3738, -11.2436,\n         -10.8022,   1.3211,  15.0990,  -1.3964,  19.6429,   9.0302, -18.7562,\n           2.8804, -11.9642,  14.8702, -37.4905,  20.4850,  22.2344,  13.4185,\n          23.4811,  -3.5016,  10.3540, -11.3863,  18.8748,  -0.7793,  28.5464,\n         -15.9820,  -5.0830,  12.7408, -19.6781,   3.1694, -37.4113,   0.6387,\n          14.9999,  -7.0661,  12.2533, -27.0358,  26.8235,   1.2522, -16.5426,\n          32.3230, -30.5527,   0.7063,  20.0358,  24.4204, -20.4096,  -6.0147,\n           8.5914,  14.3524,  12.8390,  25.4506,  -4.4919,   5.6486,   6.4814,\n          10.0003,   4.2661,   1.9836,  -4.4280,  22.0171,   1.1926,  29.6659,\n         -53.2439,  11.3238,   4.6576, -18.6014, -13.8002, -56.5608,   3.4562,\n          -3.1308, -13.3575,   4.0920,  11.9589, -32.2338,   8.8564,  -9.5333,\n           8.9081,   9.0118, -18.9721,   5.0644,  -9.7549,  -6.6916,  -4.2411,\n          -6.9300,  13.8036, -30.9491,  34.2851,  22.5484, -28.6958,  -2.1959,\n           9.8933,  -8.3126,  -7.4501,  16.5498,   8.3057,   9.1807,  -5.9104,\n         -19.7798,  -1.8471,  -6.6766,   9.0297, -16.8086, -14.7489,   2.4699,\n           2.8376, -20.3801, -44.0822, -16.2354,  26.4081,  14.7476,  -3.7843,\n           1.8738,  -9.8636,   3.0561,   1.1334,  58.4084,   4.6568,   4.7472,\n          24.9442,  23.1019, -27.2815, -13.0584,  -5.4963, -19.8954,   6.6214,\n          13.5633,  -7.3660,  16.3011,  39.1805,  13.0357,  23.8871,  16.4031,\n         -13.4699,  -4.0285,   1.3207,  13.0286,  34.3727,  34.1234,  13.8806,\n          12.6545,  33.3009, -20.3797,  -5.0877,  -9.9108,   5.8680,   3.4607,\n         -10.1464,   7.5577, -23.1309,  -3.3650, -21.7835,  20.4157,   2.7438,\n          -9.3565,   3.7427,  -4.3755,   2.2316,  -9.9273, -22.4960, -10.9746,\n          13.8108,   5.8697, -24.1893, -29.1342,  12.7063, -11.6637,  -1.8965,\n          18.0363, -10.9738,  28.6178,  13.1738, -20.6811,   6.2949, -29.2317,\n          10.4905,  25.6249,   7.1772,  -3.6654,   9.9150, -14.6621,  -3.5471,\n          30.8991, -22.9624, -13.8977, -27.6089,  10.2879,   7.5700,  14.9304,\n           4.7239,   1.8639, -18.0094,   4.2807,  -6.4345,   9.2009,   1.1207,\n          28.1937,   5.1353,   3.4480,  31.4991,  14.4123,  17.0901,   6.9207,\n          14.0292,   5.9632,  10.5258, -14.2335,  11.9814, -14.4036,   0.1918,\n         -10.3716,  12.9602, -22.4197, -30.7360, -11.3904, -14.1691, -38.2782,\n          20.8518, -18.3225, -21.0635,  18.1786,  -7.3427,  16.9819, -17.9389,\n         -26.6173, -25.7857,   7.9921, -11.9879,  12.6343, -23.1900,  52.4588,\n          -4.5111,  33.3324,  -7.5511, -21.0772,   0.3459,  19.5955,  68.8974,\n          -5.7381,  -9.8044,   4.5855,  -5.4282,  -3.1260,  12.9262,  38.1970,\n          -2.9767,  -8.3251,  18.8734, -28.4750,   4.1150,  -2.2193,   3.9797,\n          24.3802, -43.5508,  30.6544,  11.7154,   6.1890, -10.4590,   2.1950,\n         -18.6950,  -2.7942, -16.5974,   4.9551, -15.1465,  -4.5337,  -5.1674,\n          -3.4844,   3.0124,  -7.4789, -21.2037,   2.8151,  13.0681,  -0.6703,\n         -21.4589,  15.6954,   9.2001, -10.9304,  -2.0542,  23.3831, -37.0205,\n         -28.2619, -10.5926,   9.7811,  15.5613,  14.5311,   2.8618]])\n"
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"features.pickle\", \"rb\") as f:\n",
    "        print(\"Loading pickled features\")\n",
    "        G.ndata[\"feat\"] = pickle.load(f)\n",
    "except:\n",
    "    print(\"Creating feature matrix: This may take a while...\")\n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec.load(\"doc2vec.model\")\n",
    "    vectors = []\n",
    "    for article in articleToNum:\n",
    "        try: \n",
    "            words = [] \n",
    "            with open((\"plaintext_articles/\"+article+\".txt\"), \"r\") as f: \n",
    "                words = f.read().lower().split() \n",
    "            vectors.append(list(d2v_model.infer_vector(words)))\n",
    "        except:\n",
    "            vectors.append([0 for i in range(300)])\n",
    "\n",
    "    G.ndata[\"feat\"] = torch.tensor(vectors) # one hot encode nodes for features (replace with doc2vec in future)\n",
    "\n",
    "    with open(\"features.pickle\", \"wb\") as f:\n",
    "        pickle.dump(G.ndata[\"feat\"], f)\n",
    "\n",
    "print(G.nodes[2].data['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Neural Network and Labelling Relevant Verticies\n",
    "Here, we create the GCN, with options for using a two-layered GCN or a three-layered one. Thus far, the two-layered GCN appears to work better, and this is further corroborated by the fact [this](https://arxiv.org/abs/1609.02907) paper only used a two-layered one. We also label the wanted and unwanted verticies and setup the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = GCN(300, 25, 2) #Two layer GCN\n",
    "# net = threeLayerGCN(G.number_of_nodes(), 75, 25, 2) # Three Layer GCN \n",
    "\n",
    "inputs = G.ndata[\"feat\"]\n",
    "labeled_nodes = torch.tensor([wantedNum, unwantedNum])  # only the instructor and the president nodes are labeled\n",
    "labels = torch.tensor([0, 1])  # their labels are different\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Neural Network\n",
    "This loop stores the results of each epoch of training for visualization and then continues the training until the number of desired epochs is hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 0 | Loss: 3.617e-01\nEpoch 1 | Loss: 3.566e-01\nEpoch 2 | Loss: 3.515e-01\nEpoch 3 | Loss: 3.465e-01\nEpoch 4 | Loss: 3.415e-01\nEpoch 5 | Loss: 3.366e-01\nEpoch 6 | Loss: 3.317e-01\nEpoch 7 | Loss: 3.269e-01\nEpoch 8 | Loss: 3.222e-01\nEpoch 9 | Loss: 3.175e-01\nEpoch 10 | Loss: 3.129e-01\nEpoch 11 | Loss: 3.083e-01\nEpoch 12 | Loss: 3.038e-01\nEpoch 13 | Loss: 2.993e-01\nEpoch 14 | Loss: 2.949e-01\nEpoch 15 | Loss: 2.906e-01\nEpoch 16 | Loss: 2.863e-01\nEpoch 17 | Loss: 2.821e-01\nEpoch 18 | Loss: 2.779e-01\nEpoch 19 | Loss: 2.738e-01\nEpoch 20 | Loss: 2.697e-01\nEpoch 21 | Loss: 2.658e-01\nEpoch 22 | Loss: 2.618e-01\nEpoch 23 | Loss: 2.580e-01\nEpoch 24 | Loss: 2.542e-01\nEpoch 25 | Loss: 2.504e-01\nEpoch 26 | Loss: 2.467e-01\nEpoch 27 | Loss: 2.431e-01\nEpoch 28 | Loss: 2.395e-01\nEpoch 29 | Loss: 2.360e-01\nEpoch 30 | Loss: 2.326e-01\nEpoch 31 | Loss: 2.292e-01\nEpoch 32 | Loss: 2.258e-01\nEpoch 33 | Loss: 2.225e-01\nEpoch 34 | Loss: 2.193e-01\nEpoch 35 | Loss: 2.161e-01\nEpoch 36 | Loss: 2.130e-01\nEpoch 37 | Loss: 2.100e-01\nEpoch 38 | Loss: 2.069e-01\nEpoch 39 | Loss: 2.040e-01\nEpoch 40 | Loss: 2.011e-01\nEpoch 41 | Loss: 1.982e-01\nEpoch 42 | Loss: 1.954e-01\nEpoch 43 | Loss: 1.926e-01\nEpoch 44 | Loss: 1.899e-01\nEpoch 45 | Loss: 1.873e-01\nEpoch 46 | Loss: 1.846e-01\nEpoch 47 | Loss: 1.821e-01\nEpoch 48 | Loss: 1.796e-01\nEpoch 49 | Loss: 1.771e-01\nEpoch 50 | Loss: 1.746e-01\nEpoch 51 | Loss: 1.722e-01\nEpoch 52 | Loss: 1.699e-01\nEpoch 53 | Loss: 1.676e-01\nEpoch 54 | Loss: 1.653e-01\nEpoch 55 | Loss: 1.631e-01\nEpoch 56 | Loss: 1.609e-01\nEpoch 57 | Loss: 1.588e-01\nEpoch 58 | Loss: 1.567e-01\nEpoch 59 | Loss: 1.546e-01\nEpoch 60 | Loss: 1.526e-01\nEpoch 61 | Loss: 1.506e-01\nEpoch 62 | Loss: 1.486e-01\nEpoch 63 | Loss: 1.467e-01\nEpoch 64 | Loss: 1.448e-01\nEpoch 65 | Loss: 1.430e-01\nEpoch 66 | Loss: 1.411e-01\nEpoch 67 | Loss: 1.393e-01\nEpoch 68 | Loss: 1.376e-01\nEpoch 69 | Loss: 1.358e-01\nEpoch 70 | Loss: 1.342e-01\nEpoch 71 | Loss: 1.325e-01\nEpoch 72 | Loss: 1.308e-01\nEpoch 73 | Loss: 1.292e-01\nEpoch 74 | Loss: 1.277e-01\nEpoch 75 | Loss: 1.261e-01\nEpoch 76 | Loss: 1.246e-01\nEpoch 77 | Loss: 1.231e-01\nEpoch 78 | Loss: 1.216e-01\nEpoch 79 | Loss: 1.202e-01\nEpoch 80 | Loss: 1.187e-01\nEpoch 81 | Loss: 1.173e-01\nEpoch 82 | Loss: 1.160e-01\nEpoch 83 | Loss: 1.146e-01\nEpoch 84 | Loss: 1.133e-01\nEpoch 85 | Loss: 1.120e-01\nEpoch 86 | Loss: 1.107e-01\nEpoch 87 | Loss: 1.094e-01\nEpoch 88 | Loss: 1.082e-01\nEpoch 89 | Loss: 1.069e-01\nEpoch 90 | Loss: 1.057e-01\nEpoch 91 | Loss: 1.046e-01\nEpoch 92 | Loss: 1.034e-01\nEpoch 93 | Loss: 1.023e-01\nEpoch 94 | Loss: 1.011e-01\nEpoch 95 | Loss: 1.000e-01\nEpoch 96 | Loss: 9.894e-02\nEpoch 97 | Loss: 9.787e-02\nEpoch 98 | Loss: 9.682e-02\nEpoch 99 | Loss: 9.579e-02\n"
    }
   ],
   "source": [
    "all_logits = []\n",
    "for epoch in range(numEpochs):\n",
    "    logits = net(G, inputs)\n",
    "    # we save the logits for visualization later\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    # we only compute loss for labeled nodes\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch %d | Loss: %6.3e' % (epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output Predictions\n",
    "This section translates the output of the last result of training and outputs the top 10 results given the topic desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Article Id: 4\nArticle Name: Ape\nArticle Score: tensor(0.8247)\n\nArticle Id: 8\nArticle Name: Organism\nArticle Score: tensor(0.8247)\n\nArticle Id: 9\nArticle Name: Empiricism\nArticle Score: tensor(0.8247)\n\nArticle Id: 10\nArticle Name: Poor_Law\nArticle Score: tensor(0.8247)\n\nArticle Id: 13\nArticle Name: Gal%C3%A1pagos_Islands\nArticle Score: tensor(0.8247)\n\nArticle Id: 17\nArticle Name: Natural_selection\nArticle Score: tensor(0.8247)\n\nArticle Id: 19\nArticle Name: Creation-evolution_controversy\nArticle Score: tensor(0.8247)\n\nArticle Id: 21\nArticle Name: S%C3%B8ren_Kierkegaard\nArticle Score: tensor(0.8247)\n\nArticle Id: 24\nArticle Name: Sociology\nArticle Score: tensor(0.8247)\n\nArticle Id: 25\nArticle Name: Emotion\nArticle Score: tensor(0.8247)\n\n"
    }
   ],
   "source": [
    "predictions = list(all_logits[numEpochs-1])\n",
    "\n",
    "predictionsWithIndex = []\n",
    "a = 0\n",
    "for article in predictions:\n",
    "    predictionsWithIndex.append([a, article[0]])\n",
    "    a+=1\n",
    "\n",
    "predictionsWithIndex.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "topResults = predictionsWithIndex[:10]\n",
    "\n",
    "\n",
    "for article in topResults:\n",
    "    print(\"Article Id: \"+str(article[0]))\n",
    "    print(\"Article Name: \"+str(numToArticle[article[0]]))\n",
    "    print(\"Article Score: \"+str(article[1]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Credits\n",
    "- [GCN Tutorial Using DGL] (https://docs.dgl.ai/en/latest/tutorials/basics/1_first.html)\n",
    "- [Using Python With TigerGraph's REST API] (https://github.com/tigergraph/ecosys/tree/master/etl/tg-python-wrapper) (Used to create pyTigerGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}