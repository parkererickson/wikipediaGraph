{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Article Suggestion Using Graph Convolutional Networks and TigerGraph\n",
    "------\n",
    "## Introduction\n",
    "This notebook connects to a TigerGraph database that holds Wikipedia article topics and their links between them, as well as various keywords mentioned in the articles. Check out the README for more information surrounding the graph database end of the project. The purpose of this notebook is to suggest articles based upon a given topic using a Graph Convolutional Neural Network purely on the links between articles. For a deeper explaination of the GCN, check out [this](https://arxiv.org/abs/1609.02907) link."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Packages\n",
    "The core packages that need to be installed are PyTorch, dgl, and pyTigerGraph. PyTorch and dgl are used for creating and training the GCN, while pyTigerGraph is used for connecting to the TigerGraph database. We also import networkx for converting the list of edges from TigerGraph into a graph dgl can work with. gcn and cfg are other files in the code directory. gcn is already created, but you must create cfg with your TigerGraph API token. The file should look like:\n",
    "```py\n",
    "token = \"YOUR_API_TOKEN_HERE\"\n",
    "```\n",
    "To install the other packages, simply use: ```pip3 install torch dgl networkx pyTigerGraph matplotlib```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyTigerGraph as tg\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "from py_scripts.gcn import GCN\n",
    "from py_scripts.gcn import threeLayerGCN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import py_scripts.cfg as cfg\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "Here we define some variables, such as the number of epochs of training (usually only need 30 or less for a 2-layer GCN, 100+ for a 3+ layer GCN), the learning rate (0.01 seems to work well) and the topic you want recommendations based upon, as well as a completely unrelated topic. Make sure both of these topics are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 1000\n",
    "learningRate = 0.01\n",
    "wantedTopic = \"Saxophone\"\n",
    "unwantedTopic = \"Falkland_Islands\" #In future, determine automatically through the inverse of PageRank/other centrality algo"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Database Connection and Creating Edge List\n",
    "This section instantiates a connection to the TigerGraph database and creates a list of tuples with consisting of directed edges in the form of (from, to). This is done through two dictionaries that corresponds an article name to a unique numerical id that is needed to process the graph in the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tg.TigerGraphConnection(ipAddress=\"https://wikipediagraph.i.tgcloud.us\", apiToken=cfg.token, graphname=\"WikipediaGraph\") # connection to TigerGraph database\n",
    "\n",
    "articleToNum = {} # translation dictionary for article name to number (for dgl)\n",
    "numToArticle = {} # translation dictionary for number to article name\n",
    "i = 0\n",
    "def createEdgeList(result): # returns tuple of number version of edge\n",
    "    global i\n",
    "    if result[\"article1\"] in articleToNum:\n",
    "        fromKey = articleToNum[result[\"article1\"]]\n",
    "    else:\n",
    "        articleToNum[result[\"article1\"]] = i\n",
    "        numToArticle[i] = result[\"article1\"]\n",
    "        fromKey = i\n",
    "        i+=1\n",
    "    if result[\"article2\"] in articleToNum:\n",
    "        toKey = articleToNum[result[\"article2\"]]\n",
    "    else:\n",
    "        articleToNum[result[\"article2\"]] = i\n",
    "        numToArticle[i] = result[\"article2\"]\n",
    "        toKey = i\n",
    "        i+=1\n",
    "    return (fromKey, toKey)\n",
    "    \n",
    "edges = [createEdgeList(thing) for thing in conn.runInstalledQuery(\"tupleArticle\", {})[\"results\"][0][\"list_of_article_tuples\"]] # creates list of edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing Graph\n",
    "This section converts the list of edges into a graph that DGL can process in the GCN. It also converts our wanted and unwanted topics to their corresponding numerical ids that we will use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "wantedNum = articleToNum[wantedTopic]\n",
    "unwantedNum = articleToNum[unwantedTopic]\n",
    "\n",
    "g = nx.DiGraph()\n",
    "\n",
    "g.add_edges_from(edges)\n",
    "\n",
    "G = dgl.DGLGraph(g)\n",
    "\n",
    "print('We have %d nodes.' % G.number_of_nodes())\n",
    "print('We have %d edges.' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Features to Verticies in Graph\n",
    "We will use the doc2vec model to encode each of the article's into a numerical vector that can be used for the features of the vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load(\"doc2vec.model\")\n",
    "vectors = []\n",
    "for article in articleToNum:\n",
    "    try: \n",
    "        words = [] \n",
    "        with open((\"plaintext_articles/\"+article+\".txt\"), \"r\") as f: \n",
    "            words = f.read().lower().split() \n",
    "        vectors.append(list(d2v_model.infer_vector(words)))\n",
    "    except:\n",
    "        vectors.append([0 for i in range(300)])\n",
    "\n",
    "G.ndata[\"feat\"] = torch.tensor(vectors) # one hot encode nodes for features (replace with doc2vec in future)\n",
    "\n",
    "print(G.nodes[2].data['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Neural Network and Labelling Relevant Verticies\n",
    "Here, we create the GCN, with options for using a two-layered GCN or a three-layered one. Thus far, the two-layered GCN appears to work better, and this is further corroborated by the fact [this](https://arxiv.org/abs/1609.02907) paper only used a two-layered one. We also label the wanted and unwanted verticies and setup the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GCN(300, 60, 2) #Two layer GCN\n",
    "# net = threeLayerGCN(G.number_of_nodes(), 75, 25, 2) # Three Layer GCN \n",
    "\n",
    "inputs = G.ndata[\"feat\"]\n",
    "labeled_nodes = torch.tensor([wantedNum, unwantedNum])  # only the instructor and the president nodes are labeled\n",
    "labels = torch.tensor([0, 1])  # their labels are different\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Neural Network\n",
    "This loop stores the results of each epoch of training for visualization and then continues the training until the number of desired epochs is hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = []\n",
    "for epoch in range(numEpochs):\n",
    "    logits = net(G, inputs)\n",
    "    # we save the logits for visualization later\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    # we only compute loss for labeled nodes\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch %d | Loss: %6.3e' % (epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output Predictions\n",
    "This section translates the output of the last result of training and outputs the top 10 results given the topic desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(all_logits[numEpochs-1])\n",
    "\n",
    "predictionsWithIndex = []\n",
    "a = 0\n",
    "for article in predictions:\n",
    "    predictionsWithIndex.append([a, article[0]])\n",
    "    a+=1\n",
    "\n",
    "predictionsWithIndex.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "topResults = predictionsWithIndex[:10]\n",
    "\n",
    "\n",
    "for article in topResults:\n",
    "    print(\"Article Id: \"+str(article[0]))\n",
    "    print(\"Article Name: \"+str(numToArticle[article[0]]))\n",
    "    print(\"Article Score: \"+str(article[1]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Credits\n",
    "- [GCN Tutorial Using DGL] (https://docs.dgl.ai/en/latest/tutorials/basics/1_first.html)\n",
    "- [Using Python With TigerGraph's REST API] (https://github.com/tigergraph/ecosys/tree/master/etl/tg-python-wrapper) (Used to create pyTigerGraph)"
   ]
  }
 ]
}